{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data\\\\Entropy 120\\\\\"\n",
    "classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            df = pd.read_excel(file_path)\n",
    "            intensity_values = df['Intensity'].values\n",
    "            entropy_0_values = df['Entropy 0'].values\n",
    "            entropy_1_values = df['Entropy 1'].values\n",
    "            features = np.column_stack((intensity_values, entropy_0_values, entropy_1_values))\n",
    "            X.append(features)\n",
    "            for i in range(len(features)):  \n",
    "                y.append(class_idx)\n",
    "X = np.concatenate(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "[[5846 2955 1831]\n",
      " [2907 5743 1807]\n",
      " [1544 1659 7250]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.55      0.56     10632\n",
      "           1       0.55      0.55      0.55     10457\n",
      "           2       0.67      0.69      0.68     10453\n",
      "\n",
      "    accuracy                           0.60     31542\n",
      "   macro avg       0.60      0.60      0.60     31542\n",
      "weighted avg       0.60      0.60      0.60     31542\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create and train the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=2)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf)) \n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the model to disk\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "[[5760 2961 1911]\n",
      " [3473 4795 2189]\n",
      " [1431 1464 7558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.54      0.54     10632\n",
      "           1       0.52      0.46      0.49     10457\n",
      "           2       0.65      0.72      0.68     10453\n",
      "\n",
      "    accuracy                           0.57     31542\n",
      "   macro avg       0.57      0.57      0.57     31542\n",
      "weighted avg       0.57      0.57      0.57     31542\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [11:10:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes), random_state=2)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Results:\")\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "xgb_model.save_model('xgboost_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = SVC(kernel='linear')\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Evaluation\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data\\\\Entropy 120\\\\\"\n",
    "classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            df = pd.read_excel(file_path)\n",
    "            # Aggregate features from the whole file\n",
    "            intensity_values = df['Intensity'].mean()  # Mean intensity\n",
    "            entropy_0_values = df['Entropy 0'].mean()  # Mean entropy 0\n",
    "            entropy_1_values = df['Entropy 1'].mean()  # Mean entropy 1\n",
    "            features = [intensity_values, entropy_0_values, entropy_1_values]\n",
    "            X.append(features)\n",
    "            y.append(class_idx)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# # Step 2: Model Selection and Training\n",
    "# svm_model = SVC(kernel='linear')\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Evaluation\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # Step 1: Data Preprocessing\n",
    "# data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data Old\\\\Entropy\\\\\"\n",
    "# classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for class_idx, class_name in enumerate(classes):\n",
    "#     class_dir = os.path.join(data_dir, class_name)\n",
    "#     for file_name in os.listdir(class_dir):\n",
    "#         if file_name.endswith('.xlsx'):\n",
    "#             file_path = os.path.join(class_dir, file_name)\n",
    "#             df = pd.read_excel(file_path)\n",
    "#             intensity_values = df['Intensity'].values\n",
    "#             entropy_0_values = df['Entropy 0'].values\n",
    "#             entropy_1_values = df['Entropy 1'].values\n",
    "#             features = np.column_stack((intensity_values, entropy_0_values, entropy_1_values))\n",
    "#             X.append(features)\n",
    "#             for i in range(len(features)):  \n",
    "#                 y.append(class_idx)\n",
    "\n",
    "\n",
    "# X = np.concatenate(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # Splitting data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 2: Model Selection and Training\n",
    "# model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes))\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Evaluation\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n",
    "# xgb_model.save_model('xgboost_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "[[19  4  1]\n",
      " [ 7 18  2]\n",
      " [ 2  2 17]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.79      0.73        24\n",
      "           1       0.75      0.67      0.71        27\n",
      "           2       0.85      0.81      0.83        21\n",
      "\n",
      "    accuracy                           0.75        72\n",
      "   macro avg       0.76      0.76      0.76        72\n",
      "weighted avg       0.76      0.75      0.75        72\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create and train the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=2)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf)) \n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the model to disk\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "[[31 14  2]\n",
      " [20 29  5]\n",
      " [ 1  1 41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.66      0.63        47\n",
      "           1       0.66      0.54      0.59        54\n",
      "           2       0.85      0.95      0.90        43\n",
      "\n",
      "    accuracy                           0.70       144\n",
      "   macro avg       0.70      0.72      0.71       144\n",
      "weighted avg       0.70      0.70      0.70       144\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [11:02:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes), random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Results:\")\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "xgb_model.save_model('xgboost_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data\\\\Entropy\\\\\"\n",
    "classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            df = pd.read_excel(file_path)\n",
    "            intensity_values = df['Intensity'].values\n",
    "            entropy_0_values = df['Entropy 0'].values\n",
    "            entropy_1_values = df['Entropy 1'].values\n",
    "            features = np.column_stack((intensity_values, entropy_0_values, entropy_1_values))\n",
    "            X.append(features)\n",
    "            for i in range(len(features)):  \n",
    "                y.append(class_idx)\n",
    "X = np.concatenate(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "[[662 125 111]\n",
      " [ 91 662  98]\n",
      " [ 55  71 755]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78       898\n",
      "           1       0.77      0.78      0.77       851\n",
      "           2       0.78      0.86      0.82       881\n",
      "\n",
      "    accuracy                           0.79      2630\n",
      "   macro avg       0.79      0.79      0.79      2630\n",
      "weighted avg       0.79      0.79      0.79      2630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create and train the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=2)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf)) \n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the model to disk\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "[[644 139 115]\n",
      " [106 590 155]\n",
      " [ 39  63 779]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.72      0.76       898\n",
      "           1       0.74      0.69      0.72       851\n",
      "           2       0.74      0.88      0.81       881\n",
      "\n",
      "    accuracy                           0.77      2630\n",
      "   macro avg       0.77      0.76      0.76      2630\n",
      "weighted avg       0.77      0.77      0.76      2630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [11:02:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes), random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Results:\")\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "xgb_model.save_model('xgboost_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_model = SVC(kernel='linear')\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Evaluation\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data\\\\Entropy\\\\\"\n",
    "classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            df = pd.read_excel(file_path)\n",
    "            # Aggregate features from the whole file\n",
    "            intensity_values = df['Intensity'].mean()  # Mean intensity\n",
    "            entropy_0_values = df['Entropy 0'].mean()  # Mean entropy 0\n",
    "            entropy_1_values = df['Entropy 1'].mean()  # Mean entropy 1\n",
    "            features = [intensity_values, entropy_0_values, entropy_1_values]\n",
    "            X.append(features)\n",
    "            y.append(class_idx)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)\n",
    "\n",
    "# # Step 2: Model Selection and Training\n",
    "# svm_model = SVC(kernel='linear')\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Evaluation\n",
    "# y_pred = svm_model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import xgboost as xgb\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# # Step 1: Data Preprocessing\n",
    "# data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data Old\\\\Entropy\\\\\"\n",
    "# classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for class_idx, class_name in enumerate(classes):\n",
    "#     class_dir = os.path.join(data_dir, class_name)\n",
    "#     for file_name in os.listdir(class_dir):\n",
    "#         if file_name.endswith('.xlsx'):\n",
    "#             file_path = os.path.join(class_dir, file_name)\n",
    "#             df = pd.read_excel(file_path)\n",
    "#             intensity_values = df['Intensity'].values\n",
    "#             entropy_0_values = df['Entropy 0'].values\n",
    "#             entropy_1_values = df['Entropy 1'].values\n",
    "#             features = np.column_stack((intensity_values, entropy_0_values, entropy_1_values))\n",
    "#             X.append(features)\n",
    "#             for i in range(len(features)):  \n",
    "#                 y.append(class_idx)\n",
    "\n",
    "\n",
    "# X = np.concatenate(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # Splitting data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Step 2: Model Selection and Training\n",
    "# model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes))\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Step 3: Evaluation\n",
    "# y_pred = model.predict(X_test)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred, target_names=classes))\n",
    "# xgb_model.save_model('xgboost_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\aneesh pb\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "[[4 0 2]\n",
      " [0 8 0]\n",
      " [0 1 9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.67      0.80         6\n",
      "           1       0.89      1.00      0.94         8\n",
      "           2       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.88        24\n",
      "   macro avg       0.90      0.86      0.87        24\n",
      "weighted avg       0.89      0.88      0.87        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Create and train the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=2)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Random Forest Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf)) \n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the model to disk\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Results:\n",
      "[[5 0 1]\n",
      " [0 8 0]\n",
      " [2 0 8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.83      0.77         6\n",
      "           1       1.00      1.00      1.00         8\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.88        24\n",
      "   macro avg       0.87      0.88      0.87        24\n",
      "weighted avg       0.88      0.88      0.88        24\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [15:24:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Create and train the model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes), random_state=2)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"XGBoost Results:\")\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "xgb_model.save_model('xgboost_model.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Results:\n",
      "Confusion Matrix:\n",
      "[[ 93   0  19]\n",
      " [ 40   3   5]\n",
      " [ 42   0 141]]\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   Normal cases       0.53      0.83      0.65       112\n",
      "   Bengin cases       1.00      0.06      0.12        48\n",
      "Malignant cases       0.85      0.77      0.81       183\n",
      "\n",
      "       accuracy                           0.69       343\n",
      "      macro avg       0.80      0.55      0.53       343\n",
      "   weighted avg       0.77      0.69      0.66       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluation\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(\"SVM Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.3643 - loss: 2.2458 - val_accuracy: 0.1399 - val_loss: 1.9906\n",
      "Epoch 2/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3772 - loss: 1.6686 - val_accuracy: 0.5335 - val_loss: 1.1820\n",
      "Epoch 3/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4432 - loss: 1.2144 - val_accuracy: 0.3265 - val_loss: 1.4681\n",
      "Epoch 4/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4217 - loss: 1.1879 - val_accuracy: 0.3265 - val_loss: 1.5952\n",
      "Epoch 5/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4452 - loss: 1.3999 - val_accuracy: 0.5335 - val_loss: 1.6416\n",
      "Epoch 6/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4622 - loss: 1.3704 - val_accuracy: 0.3265 - val_loss: 1.4532\n",
      "Epoch 7/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5202 - loss: 1.1535 - val_accuracy: 0.1399 - val_loss: 1.3709\n",
      "Epoch 8/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3942 - loss: 1.1545 - val_accuracy: 0.5335 - val_loss: 1.0021\n",
      "Epoch 9/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4968 - loss: 1.1504 - val_accuracy: 0.5335 - val_loss: 1.0422\n",
      "Epoch 10/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4699 - loss: 1.1121 - val_accuracy: 0.5656 - val_loss: 0.9807\n",
      "Epoch 11/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4555 - loss: 1.3231 - val_accuracy: 0.3265 - val_loss: 1.3415\n",
      "Epoch 12/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4238 - loss: 1.2547 - val_accuracy: 0.5335 - val_loss: 0.9652\n",
      "Epoch 13/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5272 - loss: 0.9955 - val_accuracy: 0.3265 - val_loss: 1.4158\n",
      "Epoch 14/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4573 - loss: 1.2505 - val_accuracy: 0.5598 - val_loss: 0.9731\n",
      "Epoch 15/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4502 - loss: 1.1667 - val_accuracy: 0.5335 - val_loss: 0.9818\n",
      "Epoch 16/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.4827 - loss: 1.0806 - val_accuracy: 0.6035 - val_loss: 0.9395\n",
      "Epoch 17/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5312 - loss: 1.0422 - val_accuracy: 0.5335 - val_loss: 0.9491\n",
      "Epoch 18/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4779 - loss: 1.0900 - val_accuracy: 0.5510 - val_loss: 0.9527\n",
      "Epoch 19/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5416 - loss: 0.9940 - val_accuracy: 0.5889 - val_loss: 1.0424\n",
      "Epoch 20/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4760 - loss: 1.1291 - val_accuracy: 0.5569 - val_loss: 0.9428\n",
      "Epoch 21/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5316 - loss: 0.9910 - val_accuracy: 0.3294 - val_loss: 1.1415\n",
      "Epoch 22/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5265 - loss: 1.0630 - val_accuracy: 0.3265 - val_loss: 1.3798\n",
      "Epoch 23/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5462 - loss: 1.0466 - val_accuracy: 0.6035 - val_loss: 0.9041\n",
      "Epoch 24/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5626 - loss: 0.9956 - val_accuracy: 0.3411 - val_loss: 1.0843\n",
      "Epoch 25/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5005 - loss: 0.9821 - val_accuracy: 0.4402 - val_loss: 1.0319\n",
      "Epoch 26/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5259 - loss: 1.0305 - val_accuracy: 0.6035 - val_loss: 0.8825\n",
      "Epoch 27/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5141 - loss: 1.0262 - val_accuracy: 0.5627 - val_loss: 0.9274\n",
      "Epoch 28/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5705 - loss: 0.9256 - val_accuracy: 0.6531 - val_loss: 0.8917\n",
      "Epoch 29/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5389 - loss: 0.9438 - val_accuracy: 0.6035 - val_loss: 0.8908\n",
      "Epoch 30/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5520 - loss: 0.9359 - val_accuracy: 0.6035 - val_loss: 0.9000\n",
      "Epoch 31/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4991 - loss: 1.0297 - val_accuracy: 0.5685 - val_loss: 0.9083\n",
      "Epoch 32/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5661 - loss: 0.9350 - val_accuracy: 0.6093 - val_loss: 0.8638\n",
      "Epoch 33/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5270 - loss: 0.9825 - val_accuracy: 0.5889 - val_loss: 0.9895\n",
      "Epoch 34/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5563 - loss: 0.9778 - val_accuracy: 0.6152 - val_loss: 0.9381\n",
      "Epoch 35/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5512 - loss: 0.9533 - val_accuracy: 0.6589 - val_loss: 0.8615\n",
      "Epoch 36/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5611 - loss: 0.9313 - val_accuracy: 0.6589 - val_loss: 0.9171\n",
      "Epoch 37/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5628 - loss: 0.9388 - val_accuracy: 0.6035 - val_loss: 0.8927\n",
      "Epoch 38/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5763 - loss: 0.9395 - val_accuracy: 0.5190 - val_loss: 1.0036\n",
      "Epoch 39/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5113 - loss: 0.9573 - val_accuracy: 0.5802 - val_loss: 0.8668\n",
      "Epoch 40/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5932 - loss: 0.9187 - val_accuracy: 0.6035 - val_loss: 0.8948\n",
      "Epoch 41/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5847 - loss: 0.9329 - val_accuracy: 0.4956 - val_loss: 0.9517\n",
      "Epoch 42/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5822 - loss: 0.9157 - val_accuracy: 0.5918 - val_loss: 0.8583\n",
      "Epoch 43/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5954 - loss: 0.9170 - val_accuracy: 0.6297 - val_loss: 0.8341\n",
      "Epoch 44/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6078 - loss: 0.8791 - val_accuracy: 0.5860 - val_loss: 0.8571\n",
      "Epoch 45/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6118 - loss: 0.8957 - val_accuracy: 0.3848 - val_loss: 1.0844\n",
      "Epoch 46/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5179 - loss: 0.9634 - val_accuracy: 0.6531 - val_loss: 0.8441\n",
      "Epoch 47/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5736 - loss: 0.8955 - val_accuracy: 0.6531 - val_loss: 0.8316\n",
      "Epoch 48/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5955 - loss: 0.8934 - val_accuracy: 0.6618 - val_loss: 0.8384\n",
      "Epoch 49/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6319 - loss: 0.8749 - val_accuracy: 0.5802 - val_loss: 0.8550\n",
      "Epoch 50/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5813 - loss: 0.9064 - val_accuracy: 0.6152 - val_loss: 0.8394\n",
      "Epoch 51/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6045 - loss: 0.8739 - val_accuracy: 0.5889 - val_loss: 0.8517\n",
      "Epoch 52/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5913 - loss: 0.9161 - val_accuracy: 0.6560 - val_loss: 0.8188\n",
      "Epoch 53/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5506 - loss: 0.9398 - val_accuracy: 0.6122 - val_loss: 0.8623\n",
      "Epoch 54/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5816 - loss: 0.9068 - val_accuracy: 0.6560 - val_loss: 0.8305\n",
      "Epoch 55/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5894 - loss: 0.8742 - val_accuracy: 0.5831 - val_loss: 0.8526\n",
      "Epoch 56/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5862 - loss: 0.9051 - val_accuracy: 0.6268 - val_loss: 0.8305\n",
      "Epoch 57/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5789 - loss: 0.8751 - val_accuracy: 0.6676 - val_loss: 0.8337\n",
      "Epoch 58/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6448 - loss: 0.8542 - val_accuracy: 0.6122 - val_loss: 0.8588\n",
      "Epoch 59/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5868 - loss: 0.8943 - val_accuracy: 0.6531 - val_loss: 0.8171\n",
      "Epoch 60/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6204 - loss: 0.8956 - val_accuracy: 0.5773 - val_loss: 0.8730\n",
      "Epoch 61/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5973 - loss: 0.9000 - val_accuracy: 0.4490 - val_loss: 1.0156\n",
      "Epoch 62/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5771 - loss: 0.8936 - val_accuracy: 0.6647 - val_loss: 0.8225\n",
      "Epoch 63/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6116 - loss: 0.8781 - val_accuracy: 0.6531 - val_loss: 0.8187\n",
      "Epoch 64/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5819 - loss: 0.9024 - val_accuracy: 0.5539 - val_loss: 0.9316\n",
      "Epoch 65/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6150 - loss: 0.8859 - val_accuracy: 0.6764 - val_loss: 0.8137\n",
      "Epoch 66/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6337 - loss: 0.8579 - val_accuracy: 0.5802 - val_loss: 0.8582\n",
      "Epoch 67/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5774 - loss: 0.9237 - val_accuracy: 0.5773 - val_loss: 0.8321\n",
      "Epoch 68/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6387 - loss: 0.8435 - val_accuracy: 0.6589 - val_loss: 0.8371\n",
      "Epoch 69/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6105 - loss: 0.8541 - val_accuracy: 0.5598 - val_loss: 0.8875\n",
      "Epoch 70/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6211 - loss: 0.8478 - val_accuracy: 0.5073 - val_loss: 0.9721\n",
      "Epoch 71/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5777 - loss: 0.9213 - val_accuracy: 0.5860 - val_loss: 0.8530\n",
      "Epoch 72/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5904 - loss: 0.8805 - val_accuracy: 0.5569 - val_loss: 0.9237\n",
      "Epoch 73/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5986 - loss: 0.8819 - val_accuracy: 0.6472 - val_loss: 0.8099\n",
      "Epoch 74/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5818 - loss: 0.8673 - val_accuracy: 0.6618 - val_loss: 0.8091\n",
      "Epoch 75/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5712 - loss: 0.8984 - val_accuracy: 0.6618 - val_loss: 0.8133\n",
      "Epoch 76/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6584 - loss: 0.8164 - val_accuracy: 0.6268 - val_loss: 0.8069\n",
      "Epoch 77/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6079 - loss: 0.8577 - val_accuracy: 0.6618 - val_loss: 0.8034\n",
      "Epoch 78/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6164 - loss: 0.9043 - val_accuracy: 0.6152 - val_loss: 0.8332\n",
      "Epoch 79/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5845 - loss: 0.9159 - val_accuracy: 0.6268 - val_loss: 0.8142\n",
      "Epoch 80/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6266 - loss: 0.8409 - val_accuracy: 0.6035 - val_loss: 0.8136\n",
      "Epoch 81/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6186 - loss: 0.8543 - val_accuracy: 0.5598 - val_loss: 0.9172\n",
      "Epoch 82/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6030 - loss: 0.8643 - val_accuracy: 0.6268 - val_loss: 0.8163\n",
      "Epoch 83/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5775 - loss: 0.8379 - val_accuracy: 0.5860 - val_loss: 0.8276\n",
      "Epoch 84/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5918 - loss: 0.8574 - val_accuracy: 0.6181 - val_loss: 0.8262\n",
      "Epoch 85/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6402 - loss: 0.8301 - val_accuracy: 0.6152 - val_loss: 0.8727\n",
      "Epoch 86/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5718 - loss: 0.9212 - val_accuracy: 0.5802 - val_loss: 0.8723\n",
      "Epoch 87/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6139 - loss: 0.8878 - val_accuracy: 0.6356 - val_loss: 0.8384\n",
      "Epoch 88/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6222 - loss: 0.9022 - val_accuracy: 0.6501 - val_loss: 0.8086\n",
      "Epoch 89/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5727 - loss: 0.9195 - val_accuracy: 0.5918 - val_loss: 0.8396\n",
      "Epoch 90/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5627 - loss: 0.8687 - val_accuracy: 0.5627 - val_loss: 0.8885\n",
      "Epoch 91/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5639 - loss: 0.8975 - val_accuracy: 0.6793 - val_loss: 0.8044\n",
      "Epoch 92/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6114 - loss: 0.8662 - val_accuracy: 0.6327 - val_loss: 0.8062\n",
      "Epoch 93/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6054 - loss: 0.8331 - val_accuracy: 0.6181 - val_loss: 0.8215\n",
      "Epoch 94/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5987 - loss: 0.8516 - val_accuracy: 0.5860 - val_loss: 0.8411\n",
      "Epoch 95/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.6078 - loss: 0.8537 - val_accuracy: 0.6706 - val_loss: 0.8009\n",
      "Epoch 96/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6194 - loss: 0.8320 - val_accuracy: 0.5656 - val_loss: 0.9219\n",
      "Epoch 97/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5909 - loss: 0.8483 - val_accuracy: 0.5219 - val_loss: 1.0446\n",
      "Epoch 98/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6195 - loss: 0.8428 - val_accuracy: 0.6793 - val_loss: 0.7837\n",
      "Epoch 99/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5724 - loss: 0.9324 - val_accuracy: 0.6181 - val_loss: 0.8243\n",
      "Epoch 100/100\n",
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6170 - loss: 0.8337 - val_accuracy: 0.6589 - val_loss: 0.7986\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "Neural Network Results:\n",
      "[[ 43   0  69]\n",
      " [ 18   0  30]\n",
      " [  0   0 183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.38      0.50       112\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.65      1.00      0.79       183\n",
      "\n",
      "    accuracy                           0.66       343\n",
      "   macro avg       0.45      0.46      0.43       343\n",
      "weighted avg       0.58      0.66      0.58       343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "data_dir = \"C:\\\\Users\\\\Aneesh PB\\\\Downloads\\\\Topology Project\\\\New Data\\\\Entropy All Images\\\\\"\n",
    "classes = ['Normal cases', 'Bengin cases', 'Malignant cases']\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for class_idx, class_name in enumerate(classes):\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for file_name in os.listdir(class_dir):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            file_path = os.path.join(class_dir, file_name)\n",
    "            df = pd.read_excel(file_path)\n",
    "            # Aggregate features from the whole file\n",
    "            intensity_values = df['Intensity'].mean()  # Mean intensity\n",
    "            entropy_0_values = df['Entropy 0'].mean()  # Mean entropy 0\n",
    "            entropy_1_values = df['Entropy 1'].mean()  # Mean entropy 1\n",
    "            features = [intensity_values, entropy_0_values, entropy_1_values]\n",
    "            X.append(features)\n",
    "            y.append(class_idx)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Define the model\n",
    "nn_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(len(classes), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "nn_model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "nn_model.fit(X_train, y_train, epochs=100, batch_size=4, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_nn = np.argmax(nn_model.predict(X_test), axis=-1)\n",
    "print(\"Neural Network Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "#nn_model.save('nn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Results:\n",
      "[[ 43   0  69]\n",
      " [ 18   0  30]\n",
      " [  0   0 183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.38      0.50       112\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.65      1.00      0.79       183\n",
      "\n",
      "    accuracy                           0.66       343\n",
      "   macro avg       0.45      0.46      0.43       343\n",
      "weighted avg       0.58      0.66      0.58       343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Network Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_nn))\n",
    "print(classification_report(y_test, y_pred_nn))\n",
    "#nn_model.save('nn_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Results:\n",
      "[[ 80   9  23]\n",
      " [ 19  21   8]\n",
      " [ 21  13 149]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69       112\n",
      "           1       0.49      0.44      0.46        48\n",
      "           2       0.83      0.81      0.82       183\n",
      "\n",
      "    accuracy                           0.73       343\n",
      "   macro avg       0.66      0.66      0.66       343\n",
      "weighted avg       0.73      0.73      0.73       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# Create and train the KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (k)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"KNN Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(classification_report(y_test, y_pred_knn)) \n",
    "\n",
    "# Save the model to disk\n",
    "with open('knn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(knn_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Results:\n",
      "[[112   0   0]\n",
      " [ 45   3   0]\n",
      " [150   0  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      1.00      0.53       112\n",
      "           1       1.00      0.06      0.12        48\n",
      "           2       1.00      0.18      0.31       183\n",
      "\n",
      "    accuracy                           0.43       343\n",
      "   macro avg       0.79      0.41      0.32       343\n",
      "weighted avg       0.79      0.43      0.35       343\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# Create and train the Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Naive Bayes Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb)) \n",
    "\n",
    "# Save the model to disk\n",
    "with open('naive_bayes_model.pkl', 'wb') as f:\n",
    "    pickle.dump(nb_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "[[ 79   0  33]\n",
      " [ 40   0   8]\n",
      " [ 34   0 149]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.71      0.60       112\n",
      "           1       0.00      0.00      0.00        48\n",
      "           2       0.78      0.81      0.80       183\n",
      "\n",
      "    accuracy                           0.66       343\n",
      "   macro avg       0.43      0.51      0.47       343\n",
      "weighted avg       0.59      0.66      0.62       343\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Aneesh PB\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pickle\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "log_reg_model = LogisticRegression(random_state=2)\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = log_reg_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr)) \n",
    "\n",
    "# Save the model to disk\n",
    "with open('logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(log_reg_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Create and train the model\n",
    "# svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"SVM Results:\")\n",
    "# print(confusion_matrix(y_test, y_pred_svm))\n",
    "# print(classification_report(y_test, y_pred_svm))\n",
    "# from joblib import dump, load\n",
    "\n",
    "# # Save the model to disk\n",
    "# dump(svm_model, 'svm_model.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rbf = SVC(kernel='rbf', gamma=0.5, C=0.1).fit(X_train, y_train)\n",
    "# poly = SVC(kernel='poly', degree=3, C=1).fit(X_train, y_train)\n",
    "\n",
    "# y_pred_svm_rbf = rbf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"SVM Results:\")\n",
    "# print(confusion_matrix(y_test, y_pred_svm_rbf))\n",
    "# print(classification_report(y_test, y_pred_svm_rbf))\n",
    "# from joblib import dump, load\n",
    "\n",
    "# # Save the model to disk\n",
    "# dump(svm_model, 'svm_model_rbf.joblib')\n",
    "\n",
    "# y_pred_svm_poly = poly.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"SVM Results:\")\n",
    "# print(confusion_matrix(y_test, y_pred_svm_poly))\n",
    "# print(classification_report(y_test, y_pred_svm_poly))\n",
    "# from joblib import dump, load\n",
    "\n",
    "# # Save the model to disk\n",
    "# dump(svm_model, 'svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# # Define the model\n",
    "# nn_model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     Dense(64, activation='relu'),\n",
    "#     Dense(len(classes), activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# nn_model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# nn_model.fit(X_train, y_train, epochs=1000, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluate the model\n",
    "# y_pred_nn = np.argmax(nn_model.predict(X_test), axis=-1)\n",
    "# print(\"Neural Network Results:\")\n",
    "# print(confusion_matrix(y_test, y_pred_nn))\n",
    "# print(classification_report(y_test, y_pred_nn))\n",
    "# nn_model.save('nn_model.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# # Create individual classifiers\n",
    "# rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(classes), random_state=42)\n",
    "# svm_classifier = SVC(kernel='linear', C=1.0, probability=True, random_state=42)\n",
    "\n",
    "# # Create a voting classifier\n",
    "# voting_clf = VotingClassifier(estimators=[\n",
    "#     ('random_forest', rf_classifier),\n",
    "#     ('xgboost', xgb_classifier),\n",
    "#     ('svm', svm_classifier)\n",
    "# ], voting='soft') # 'soft' voting for probabilities\n",
    "\n",
    "# # Train the voting classifier\n",
    "# voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Voting Classifier Results:\")\n",
    "# print(confusion_matrix(y_test, y_pred_voting))\n",
    "# print(classification_report(y_test, y_pred_voting))\n",
    "\n",
    "# with open('random_forest_model1.pkl', 'wb') as f:\n",
    "#     pickle.dump(rf_classifier, f)\n",
    "\n",
    "# xgb_classifier.save_model('xgboost_model1.model')\n",
    "\n",
    "# dump(svm_classifier, 'svm_model1.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# # Data Preparation\n",
    "# # X = df['Intensity']\n",
    "# # y = df[['Entropy 0', 'Entropy 1']]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the XGBoost classifier\n",
    "# clf = XGBClassifier()\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500],\n",
    "#     'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "#     'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45],\n",
    "#     'min_child_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#     'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#     'subsample': [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     'colsample_bytree': [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0],\n",
    "#     'reg_alpha': [0, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0],\n",
    "#     'reg_lambda': [0, 0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters found\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Predict on the test set using the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "# y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "\n",
    "# # Best Parameters\n",
    "# # best_params = grid_search.best_params_\n",
    "\n",
    "# # # Model Training\n",
    "# # best_model = XGBRegressor(**best_params)\n",
    "# # best_model.fit(X_train, y_train)\n",
    "\n",
    "# # # Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the XGBoost classifier\n",
    "# clf = xgb.XGBClassifier()\n",
    "\n",
    "# # Define the parameter grid to search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 400],\n",
    "#     'max_depth': [3, 5, 7, 9],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'min_child_weight': [1, 3, 5, 7],\n",
    "#     'gamma': [0, 0.1, 0.2, 0.3],\n",
    "#     'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     'reg_alpha': [0, 0.1, 0.5, 1.0],\n",
    "#     'reg_lambda': [0, 0.1, 0.5, 1.0]\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters found\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Predict on the test set using the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "# y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 400, 500],\n",
    "#     'max_depth': [3, 5, 7, 9, 11],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2, 0.3, 0.4],\n",
    "#     'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#     'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "#     'min_child_weight': [1, 3, 5, 7, 9],\n",
    "#     'reg_alpha': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "#     'reg_lambda': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "#     #'scale_pos_weight': [1, 2, 3, 4, 5]\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters found\n",
    "# print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# # Predict on the test set using the best model\n",
    "# best_clf = grid_search.best_estimator_\n",
    "# y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # mse = mean_squared_error(y_test, best_model.predict(X_test))\n",
    "# # print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# # Make predictions\n",
    "# y_pred_xgb = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"XGBoost Results:\")\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred_xgb))\n",
    "# clf.save_model('xgboost_gridsearch_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load individual models\n",
    "# with open('random_forest_model.pkl', 'rb') as f:\n",
    "#     loaded_rf_classifier = pickle.load(f)\n",
    "\n",
    "# loaded_xgb_classifier = xgb.XGBClassifier()\n",
    "# loaded_xgb_classifier.load_model('xgboost_model.model')\n",
    "\n",
    "# loaded_svm_classifier = load('svm_model.joblib')\n",
    "\n",
    "# # Create a new VotingClassifier with loaded models\n",
    "# loaded_voting_clf = VotingClassifier(estimators=[\n",
    "#     ('random_forest', loaded_rf_classifier),\n",
    "#     ('xgboost', loaded_xgb_classifier),\n",
    "#     ('svm', loaded_svm_classifier)\n",
    "# ], voting='soft')\n",
    "\n",
    "# # Make predictions using the loaded voting classifier\n",
    "# y_pred_loaded_voting = loaded_voting_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
